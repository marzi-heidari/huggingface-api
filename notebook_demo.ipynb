{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Parallel Inference Demo: HuggingFace Sentiment Analysis API\n",
    "\n",
    "This notebook demonstrates how to send concurrent POST requests to a locally deployed FastAPI server that wraps a HuggingFace model. The goal is to validate the server's ability to handle parallel inference requests efficiently."
   ],
   "id": "d53af8b8d4bbb63e"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-30T19:51:21.870328Z",
     "start_time": "2025-05-30T19:51:21.786143Z"
    }
   },
   "source": [
    "import requests\n",
    "import concurrent.futures"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step 1: Define the API Endpoint\n",
    "\n",
    "Since the service is deployed using Docker Compose, NGINX listens on port `80` and routes incoming requests to the FastAPI app. On the host machine, this is accessible via:"
   ],
   "id": "9b61cec3d1f099da"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-30T21:01:25.101305Z",
     "start_time": "2025-05-30T21:01:25.098692Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Endpoint URL\n",
    "API_URL = \"http://localhost/predict\""
   ],
   "id": "5e0d7586354406f5",
   "outputs": [],
   "execution_count": 48
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step 2: Prepare Input Texts\n",
    "\n",
    "We define a small batch of example sentences for sentiment classification. These texts will be sent in parallel to simulate concurrent user requests.\n"
   ],
   "id": "2c19d9f4d0bfd544"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-30T21:01:26.880122Z",
     "start_time": "2025-05-30T21:01:26.875943Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Sample inputs\n",
    "texts = [\n",
    "    \"I love this!\",\n",
    "    \"This is terrible.\",\n",
    "    \"Fantastic experience.\",\n",
    "    \"Horrible and boring.\",\n",
    "    \"Absolutely amazing!\",\n",
    "    \"Not worth watching.\",\n",
    "    \"I enjoyed it a lot.\",\n",
    "    \"Disappointing outcome.\"\n",
    "]"
   ],
   "id": "7977793e116a5822",
   "outputs": [],
   "execution_count": 49
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step 3: Define the Request Function\n",
    "\n",
    "This function sends a POST request to the `/predict` endpoint with the given text and returns the server's response."
   ],
   "id": "c1be64cecb0a6dbd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-30T21:01:47.340673Z",
     "start_time": "2025-05-30T21:01:47.338417Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Function to send POST request\n",
    "def send_request(text):\n",
    "    try:\n",
    "        response = requests.post(API_URL, json={\"text\": text}, timeout=10)\n",
    "        return response.json()\n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e)}\n"
   ],
   "id": "4b2750e73db724be",
   "outputs": [],
   "execution_count": 50
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step 4: Send Requests in Parallel\n",
    "\n",
    "We use Python's `ThreadPoolExecutor` to simulate multiple users sending requests at the same time. This helps assess the server's concurrency capabilities under load.\n"
   ],
   "id": "6817598d976c5bf9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-30T21:01:59.251211Z",
     "start_time": "2025-05-30T21:01:58.686931Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Send requests in parallel\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=1) as executor:\n",
    "    futures = [executor.submit(send_request, text) for text in texts]\n",
    "    for future in concurrent.futures.as_completed(futures):\n",
    "        print(future.result())"
   ],
   "id": "73ec70b0e7f43a7a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'result': [{'label': 'POSITIVE', 'score': 0.9998764991760254}]}\n",
      "{'result': [{'label': 'NEGATIVE', 'score': 0.9996345043182373}]}\n",
      "{'result': [{'label': 'POSITIVE', 'score': 0.999881386756897}]}\n",
      "{'result': [{'label': 'NEGATIVE', 'score': 0.9997796416282654}]}\n",
      "{'result': [{'label': 'POSITIVE', 'score': 0.9998759031295776}]}\n",
      "{'result': [{'label': 'NEGATIVE', 'score': 0.9997840523719788}]}\n",
      "{'result': [{'label': 'POSITIVE', 'score': 0.9998775720596313}]}\n",
      "{'result': [{'label': 'NEGATIVE', 'score': 0.9997884631156921}]}\n"
     ]
    }
   ],
   "execution_count": 52
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step 5: Conclusion\n",
    "\n",
    "Each printed output shows the sentiment prediction (label and confidence) for an input text. This confirms the API is functioning correctly under concurrent access.\n",
    "\n",
    "This setup can be easily scaled or extended for further stress testing using tools like `locust` or `wrk` in a production environment.\n"
   ],
   "id": "69caaeaf7a23ba13"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "acfb3cb8ff81fb7a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
